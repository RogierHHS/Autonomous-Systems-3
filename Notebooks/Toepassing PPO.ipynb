{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54186d12",
   "metadata": {},
   "source": [
    "# Toepassing PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5f7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install stable_baselines3\n",
    "#!pip install gymnasium[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b730e3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bosch\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\bosch\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO) agent for the Atari Warlords environment.\n",
    "\n",
    "    Attributes:\n",
    "        env_name (str): Name of the gym environment.\n",
    "        env: The instantiated gym environment.\n",
    "        model: The PPO model from stable-baselines3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_name=\"ALE/Warlords-v5\"):\n",
    "        \"\"\"\n",
    "        Initializes the PPOAgent with a specified environment.\n",
    "\n",
    "        Parameters:\n",
    "            env_name (str): Gym environment name. Defaults to \"ALE/Warlords-v5\".\n",
    "        \"\"\"\n",
    "        self.env = gym.make(env_name)\n",
    "        # CnnPolicy, want de input zijn afbeeldingen\n",
    "        self.model = PPO(\n",
    "            \"CnnPolicy\", \n",
    "            self.env, \n",
    "            verbose=1,\n",
    "            learning_rate=2.5e-4,\n",
    "            gamma=0.99,\n",
    "            n_steps=128,\n",
    "            batch_size=256,\n",
    "            ent_coef=0.01\n",
    "        )\n",
    "        self.env_name = env_name\n",
    "\n",
    "    def train(self, timesteps=100_000):\n",
    "        \"\"\"\n",
    "        Train the PPO model for a specified number of timesteps.\n",
    "\n",
    "        Parameters:\n",
    "            timesteps (int): Number of training timesteps.\n",
    "        \"\"\"\n",
    "        self.model.learn(total_timesteps=timesteps)\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current observation using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "            observation (np.ndarray): Current environment observation.\n",
    "\n",
    "        Returns:\n",
    "            int: Chosen action.\n",
    "        \"\"\"\n",
    "        action, _ = self.model.predict(observation, deterministic=True)\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"ppo_warlords.zip\"):\n",
    "        \"\"\"\n",
    "        Saves the trained model to a file.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load(self, path=\"ppo_warlords.zip\"):\n",
    "        \"\"\"\n",
    "        Loads a trained model from a file.\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Path to the saved model.\n",
    "        \"\"\"\n",
    "        self.model = PPO.load(path, env=self.env)\n",
    "\n",
    "    def evaluate(self, episodes=5):\n",
    "        \"\"\"\n",
    "        Evaluates the PPO agent for a given number of episodes and prints the total reward.\n",
    "\n",
    "        Parameters:\n",
    "            episodes (int): Number of evaluation episodes.\n",
    "        \"\"\"\n",
    "        env = gym.make(self.env_name)\n",
    "        for ep in range(episodes):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.act(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                env.render()\n",
    "            print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594a52",
   "metadata": {},
   "source": [
    "### Uitleg van PPO\n",
    "\n",
    "Proximal Policy Optimization (PPO) is een geavanceerd reinforcement learning-algoritme dat veel wordt gebruikt voor het trainen van agents in complexe omgevingen zoals Atari-spellen. PPO verbetert eerdere policy-gradient-methodes door bij elke stap de aanpassing van het beleid (“policy”) te beperken. Hierdoor wordt het leerproces stabieler en is de kans kleiner dat de agent ineens “vergeet” wat hij geleerd heeft.\n",
    "\n",
    "PPO werkt door het beleid steeds een klein beetje aan te passen op basis van ervaringen uit de omgeving. Hierdoor leert de agent efficiënter en zijn de resultaten vaak beter reproduceerbaar. PPO is ook geschikt voor situaties met hoge-dimensionale input, zoals beelden, en werkt goed in multi-agent omgevingen zoals Warlords.\n",
    "\n",
    "DhanushKumar (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e27021",
   "metadata": {},
   "source": [
    "### Motivatie\n",
    "\n",
    "Voor deze opdracht heb ik gekozen voor het algoritme Proximal Policy Optimization (PPO). De belangrijkste reden hiervoor is dat PPO bekend staat om zijn stabiliteit en efficiëntie bij het trainen van agents in complexe omgevingen met hoge-dimensionale input, zoals de Atari-game Warlords, Schulman et al. (2017). Omdat PPO de aanpassingen aan het beleid per stap beperkt, blijft het leerproces gecontroleerd en voorkom je dat de agent tijdens het trainen “vergeet” wat eerder geleerd is. Dit is vooral belangrijk in multi-agent omgevingen, waar het gedrag van andere agents de situatie voortdurend beïnvloedt.\n",
    "\n",
    "Daarnaast is PPO eenvoudig te implementeren dankzij bestaande libraries zoals stable-baselines3, waardoor het mogelijk is om snel te experimenteren met verschillende hyperparameters. De standaardwaarden die ik voor de belangrijkste hyperparameters heb gekozen, zijn gebaseerd op aanbevelingen uit de literatuur en eerdere succesvolle toepassingen in vergelijkbare omgevingen (The 37 Implementation Details Of Proximal Policy Optimization · The ICLR Blog Track, 2022). PPO is bovendien goed schaalbaar, waardoor het geschikt is om in een multi-agent setting zoals Warlords verschillende agents onafhankelijk van elkaar te trainen en te vergelijken.\n",
    "\n",
    "Door deze eigenschappen is PPO naar mijn mening de meest geschikte keuze voor deze opdracht, omdat het zorgt voor betrouwbare leerresultaten en flexibiliteit biedt bij het uitvoeren van experimenten met verschillende agents en trainingsinstellingen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1e885",
   "metadata": {},
   "source": [
    "### Aanpak\n",
    "\n",
    "Voor deze opdracht heb ik een PPO-agent geïmplementeerd met behulp van de stable-baselines3 library. PPO is gekozen vanwege de stabiele prestaties en de robuustheid bij het trainen in omgevingen met beeldinput, zoals “Warlords”. De agent gebruikt een convolutioneel neuraal netwerk om de observaties te verwerken.\n",
    "De belangrijkste hyperparameters zijn gekozen op basis van aanbevolen waarden uit wetenschappelijke literatuur voor Atari-omgevingen, maar kunnen verder geoptimaliseerd worden.\n",
    "Mijn implementatie maakt het makkelijk om het model op te slaan, opnieuw te laden en te evalueren, zodat experimenten goed reproduceerbaar zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05c436",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "##### Overzicht keuzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c05ca",
   "metadata": {},
   "source": [
    "| Hyperparameter        | Waarde    | Uitleg                                                                                 |\n",
    "|----------------------|-----------|----------------------------------------------------------------------------------------|\n",
    "| `Policy`              | CnnPolicy | Gebruikt convolutieneurale netwerken, geschikt voor beeldinput zoals bij Atari-games   |\n",
    "| `learning_rate`        | 2.5e-4    | Standaardwaarde voor PPO in Atari-omgevingen; zorgt voor een goede balans tussen snel leren en stabiliteit |\n",
    "| `gamma`                | 0.99      | Kortingfactor; waardeert toekomstige beloningen bijna net zo hoog als directe beloning |\n",
    "| `n_steps`              | 128       | Aantal stappen per update; bepaalt hoeveel ervaringen per batch worden verzameld       |\n",
    "| `batch_size`           | 256       | Grootte van elke batch die wordt gebruikt tijdens het updaten van het beleid           |\n",
    "| `ent_coef (entropie)`  | 0.01      | Coëfficiënt voor exploratie; hogere waarde zorgt voor meer exploratiegedrag            |\n",
    "| `verbose`              | 1         | Zorgt voor gedetailleerde logging tijdens het trainen                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c620b",
   "metadata": {},
   "source": [
    "##### Experimenteren met hyperparameters\n",
    "\n",
    "Tijdens het trainen heb ik geëxperimenteerd met verschillende waarden voor de learning rate, entropie-coëfficiënt en batch size:\n",
    "- Learning rate: Een hogere learning rate zorgde voor snellere training, maar maakte het model soms instabiel. 2.5e-4 bleek een goed compromis.\n",
    "- Entropie-coëfficiënt (ent_coef): Met een hogere waarde ging de agent meer experimenteren, maar duurde het langer voordat hij goed leerde. 0.01 gaf een goede balans.\n",
    "- Batch size: Grotere batches zorgden voor stabielere updates, maar vroegen meer geheugen.\n",
    "\n",
    "De uiteindelijke hyperparameterkeuzes zijn gebaseerd op wat het beste werkte voor deze specifieke omgeving en op basis van literatuur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadfb2f",
   "metadata": {},
   "source": [
    "### Literatuur\n",
    "\n",
    "- DhanushKumar. (2024, 12 mei). PPO Algorithm - DhanushKumar - Medium. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017, 20 juli). Proximal Policy optimization Algorithms. arXiv.org. https://arxiv.org/abs/1707.06347\n",
    "- The 37 Implementation Details of Proximal Policy Optimization · The ICLR Blog Track. (2022, 25 maart). https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/?utm_source=chatgpt.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
