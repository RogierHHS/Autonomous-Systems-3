{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e982cc9",
      "metadata": {
        "id": "4e982cc9"
      },
      "source": [
        "<div style=\"background-color:LightBlue; text-align:center; padding:20px;\">\n",
        "    <h2 style=\"color:black; font-family: Verdana, sans-serif;\"><strong>Multi-Agent Reinforcement Learning Project - Atari</strong></h2>\n",
        "    <p style=\"font-size: 14px; color: black; font-family: Verdana, sans-serif;\">\n",
        "        <table style=\"margin: auto; border-collapse: collapse; color: black;\">\n",
        "            <tr>\n",
        "                <th style=\"border: 0;\">Names</th>\n",
        "                <th style=\"border: 0;\">GitHub Username</th>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"border: 0;\">Rogier Gernaat</td>\n",
        "                <td style=\"border: 0;\">RogierHHS</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"border: 0;\">Daan Eising</td>\n",
        "                <td style=\"border: 0;\">DaanEising</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"border: 0;\">Julia Boschman</td>\n",
        "                <td style=\"border: 0;\">JuliaBoschman</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"border: 0;\">Jort dihhhoek</td>\n",
        "                <td style=\"border: 0;\">Homoboi-kankerboi</td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </p></div>\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; align-items: center; margin-top: 10px;\">\n",
        "    <img src=\"\" alt=\"fotoj van onze opdracht\" style=\"width: 1000px; height: auto;\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0851f3ab",
      "metadata": {
        "id": "0851f3ab"
      },
      "source": [
        "- ***Docent***: Vikram Radhakrishnan\n",
        "- ***Datum***: 08-04-2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57553bbf",
      "metadata": {
        "id": "57553bbf"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong> Inhoudsopgave (Prototype morgen kunnen we hem nog ff aanpassen of anders indelen) </strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7dc902d",
      "metadata": {
        "id": "d7dc902d"
      },
      "source": [
        "## **Inhoudsopgave**\n",
        "\n",
        "1. [H1: Inleiding](#1.0)\n",
        "   - [&sect;1.1: Imports en Setup](#1.1)  \n",
        "  \n",
        "2. [H2: Kiezen van Algoritme](#2.0)  \n",
        "   - [&sect;2.1: Kiezen van RL-Algoritme](#2.1)    \n",
        "   - [&sect;2.3: Kiezen van Trainingsstrategie](#2.3)  \n",
        "\n",
        "3. [H3: Probleemdefinitie](#3.0)  \n",
        "   - [&sect;3.1: Wat is het probleem?](#3.1)   \n",
        "\n",
        "4. [H4: Ontwerp en Implementatie](#4.0)  \n",
        "   - [&sect;4.1: Baseline strategie ontwikkelen](#4.1)  \n",
        "   - [&sect;4.2: Selectie van DRL algoritme en frameworks](#4.2)   \n",
        "   - [&sect;4.3: Implementatie MARL-agent](#4.3)  \n",
        "\n",
        "5. [H5: Training en Hyperparameter Search](#5.0)  \n",
        "   - [&sect;5.1: Training](#5.1)  \n",
        "   - [&sect;5.2: Selectie en tuning van hyperparameters](#5.2)  \n",
        "\n",
        "6. [H6: Evaluatie en Vergelijking](#6.0)  \n",
        "   - [&sect;6.1: Evaluatie t.o.v. baseline](#6.1)  \n",
        "   - [&sect;6.2: Analyse met metrics](#6.2)  \n",
        "   - [&sect;6.3: Visualisatie van resultaten](#6.3)  \n",
        "\n",
        "7. [H7: Rapportage en Reflectie](#7.0)  \n",
        "   - [&sect;7.1: Methodologie en aanpak](#7.1)  \n",
        "   - [&sect;7.2: Samenvatting van resultaten](#7.2)  \n",
        "   - [&sect;7.3: Reflectie op model, prestaties en uitbreidingsmogelijkheden](#7.3)  \n",
        "\n",
        "8. [H8: Literatuurlijst](#8.0)  \n",
        "\n",
        "9. [H9: Beoordelingscriteria](#9.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59f4614",
      "metadata": {
        "id": "b59f4614"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H1: Inleiding </strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4151918",
      "metadata": {
        "id": "d4151918"
      },
      "source": [
        "## inleiding bladiebladiebla"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7cc6769",
      "metadata": {
        "id": "b7cc6769"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;1.1: Imports en Setup</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Pip install's</strong>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "L4NIctHLp1oS"
      },
      "id": "L4NIctHLp1oS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "\n",
        "!pip install gym\n",
        "!pip install stable_baselines3\n",
        "!pip install gymnasium[atari]\n",
        "!pip install pettingzoo[atari]\n",
        "!pip install \"autorom[accept-rom-license]\"\n",
        "!pip install --find-links dist/ --no-cache-dir AutoROM[accept-rom-license]\n",
        "!pip install supersuit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULHKlbghpy0k",
        "outputId": "1986e130-1723-40c3-8f12-7ad2a8e89fef"
      },
      "id": "ULHKlbghpy0k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.6.0\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.11.1)\n",
            "Collecting pettingzoo[atari]\n",
            "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (1.1.1)\n",
            "Collecting multi_agent_ale_py>=0.1.11 (from pettingzoo[atari])\n",
            "  Downloading multi-agent-ale-py-0.1.11.tar.gz (551 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.0/552.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[atari]) (0.0.4)\n",
            "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: multi_agent_ale_py\n",
            "  Building wheel for multi_agent_ale_py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multi_agent_ale_py: filename=multi_agent_ale_py-0.1.11-cp311-cp311-linux_x86_64.whl size=721821 sha256=d7fed3ae3618d2ed568e466018ffa2df0b159ed5385d290e18a1bd194b53e151\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/81/76/771ec8e34292c8a71dd6c4a52a1c0401f4d93cbfb54e02fce4\n",
            "Successfully built multi_agent_ale_py\n",
            "Installing collected packages: multi_agent_ale_py, pettingzoo\n",
            "Successfully installed multi_agent_ale_py-0.1.11 pettingzoo-1.25.0\n",
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.6.15)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=637e81a92fa45e5a26c692c89f299ce38c6cd850776f31f9d291a6f744208101\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n",
            "Looking in links: dist/\n",
            "Requirement already satisfied: AutoROM[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from AutoROM[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->AutoROM[accept-rom-license]) (2025.6.15)\n",
            "Collecting supersuit\n",
            "  Downloading supersuit-3.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.1.1)\n",
            "Collecting tinyscaler>=1.2.6 (from supersuit)\n",
            "  Downloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->supersuit) (0.0.4)\n",
            "Downloading supersuit-3.10.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyscaler-1.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tinyscaler, supersuit\n",
            "Successfully installed supersuit-3.10.0 tinyscaler-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e99777",
      "metadata": {
        "id": "12e99777"
      },
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Importeren van de library's</strong>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593b2447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "593b2447",
        "outputId": "c3b1ddb7-7005-45a9-a796-2ef45b123f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
            "\t/usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "\n",
            "I own a license to these Atari 2600 ROMs.\n",
            "I agree to not distribute these ROMs and wish to proceed: [Y/n]: Y\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/et.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from google.colab import drive\n",
        "# Start AutoROM\n",
        "\n",
        "!AutoROM\n",
        "\n",
        "# Import libraries\n",
        "from pettingzoo.atari import warlords_v3\n",
        "from pettingzoo.utils import BaseParallelWrapper\n",
        "from collections import defaultdict, Counter\n",
        "import importlib\n",
        "import os\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Mounten van colab drive en inladen van de agents</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "041sv7N4qs7r"
      },
      "id": "041sv7N4qs7r"
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w-qIX5DqtDb",
        "outputId": "a65bea9e-c8c7-4fb7-d30b-7326c44955ce"
      },
      "id": "5w-qIX5DqtDb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/Autonomous Systems')"
      ],
      "metadata": {
        "id": "lL5Tqu-EBjYQ"
      },
      "id": "lL5Tqu-EBjYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('/content/drive/MyDrive/Autonomous Systems'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xZNZi63C5Kn",
        "outputId": "9ad7e262-73ca-42c6-a27b-e59cdc28240c"
      },
      "id": "0xZNZi63C5Kn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438c13d8",
      "metadata": {
        "id": "438c13d8"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H2: Kiezen van Algoritme </strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732b4263",
      "metadata": {
        "id": "732b4263"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;2.1: Kiezen van RL-Algoritme</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c1c9536",
      "metadata": {
        "id": "6c1c9536"
      },
      "source": [
        "a.\tJouw agent zal deelnemen en interacten met andere agenten in de Atari-omgeving \"Warlords\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Agent 1: (Rogier) DQN - Deep Q-Network</strong>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "WRwBHRpV864G"
      },
      "id": "WRwBHRpV864G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## §2.1: Kiezen van RL-Algoritme\n",
        "\n",
        "In deze opdracht staat de Atari-game **Warlords** centraal: een klassieke arcadegame waarbij vier spelers (agents) gelijktijdig strijden op een speelveld. Iedere agent verdedigt zijn eigen kasteel en probeert de anderen uit te schakelen, wat resulteert in een typisch **multi-agent scenario** met zowel competitie als wisselende interactiepatronen tussen agents. Voor deze complexe en dynamische omgeving is het belangrijk om een reinforcement learning-algoritme te kiezen dat bewezen effectief is bij problemen met een hoge mate van onzekerheid, veel mogelijke toestanden en meerdere spelers.\n",
        "\n",
        "We kiezen voor het **Proximal Policy Optimization (PPO)** algoritme als basis voor onze agent. PPO is een krachtig, modern en veelgebruikt algoritme binnen deep reinforcement learning. Het is ontworpen voor stabiliteit en efficiëntie bij het optimaliseren van beleid (policies), en heeft uitstekende prestaties laten zien in visuele, dynamische omgevingen zoals Atari-games. PPO leert direct van ruwe pixeldata via een convolutioneel neuraal netwerk en maakt gebruik van policy-gradient updates die gecontroleerd worden uitgevoerd om instabiliteit te voorkomen.\n",
        "\n",
        "### Waarom PPO?\n",
        "\n",
        "- **Geschikt voor visuele input:**  \n",
        "  PPO werkt uitstekend met ruwe spelbeelden (frames) als input, en kan daardoor zelfstandig complexe strategieën ontwikkelen zonder handmatige feature engineering. De convolutionele neurale netwerken van PPO zijn uitermate geschikt voor het herkennen van visuele patronen in Atari-omgevingen.\n",
        "\n",
        "- **Stabiel en robuust leren:**  \n",
        "  Door het gebruik van trust-region updates (clipping), mini-batch learning en advantage schatting (GAE), blijft het leerproces gecontroleerd en raakt het model minder snel verstrikt in abrupte of onstabiele policy-wijzigingen. Dit is cruciaal in chaotische multi-agent omgevingen zoals Warlords.\n",
        "\n",
        "- **Multi-agent compatibiliteit:**  \n",
        "  PPO laat zich eenvoudig toepassen op multi-agent settings, bijvoorbeeld via parameter sharing (één policy voor meerdere agents) of door individuele policies te trainen per agent. Dit maakt het flexibel inzetbaar voor uiteenlopende experimenten.\n",
        "\n",
        "- **Breed onderzocht en veel gebruikt:**  \n",
        "  PPO behoort tot de standaardbenchmarks in reinforcement learning en is in tal van studies succesvol ingezet voor Atari-omgevingen en multi-agent settings. Er zijn veel goed onderhouden frameworks beschikbaar (zoals Stable-Baselines3 en PettingZoo), wat snelle en correcte implementatie mogelijk maakt.\n",
        "\n",
        "### Concreet voordeel voor deze opdracht\n",
        "\n",
        "- **Algoritmische robuustheid:**  \n",
        "  PPO is bijzonder effectief in dynamische en onvoorspelbare omgevingen waar het gedrag van andere agents voortdurend verandert. Dankzij gecontroleerde policy-updates en efficiënte verwerking van ervaringen is PPO in staat om robuuste strategieën te ontwikkelen.\n",
        "\n",
        "- **Vergelijkbaarheid:**  \n",
        "  PPO is een veelgebruikte standaard in de literatuur rondom Atari en multi-agent reinforcement learning. Hierdoor zijn onze resultaten goed te vergelijken met bestaande benchmarks en alternatieve algoritmes (zoals DQN of random policies).\n",
        "\n",
        "- **Transparantie en reproduceerbaarheid:**  \n",
        "  Dankzij de brede adoptie en solide implementaties in frameworks als Stable-Baselines3 en PettingZoo zijn onze experimenten eenvoudig te reproduceren, uit te breiden en te valideren door andere studenten of onderzoekers.\n"
      ],
      "metadata": {
        "id": "aTDt5gr586-3"
      },
      "id": "aTDt5gr586-3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Agent 2: (Julia) PPO - Proximal Policy Optimization</strong>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "Hth3awtZ7i_v"
      },
      "id": "Hth3awtZ7i_v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uitleg van PPO**\n",
        "\n",
        "Proximal Policy Optimization (PPO) is een geavanceerd reinforcement learning-algoritme dat veel wordt gebruikt voor het trainen van agents in complexe omgevingen zoals Atari-spellen. PPO verbetert eerdere policy-gradient-methodes door bij elke stap de aanpassing van het beleid (“policy”) te beperken. Hierdoor wordt het leerproces stabieler en is de kans kleiner dat de agent ineens “vergeet” wat hij geleerd heeft.\n",
        "\n",
        "PPO werkt door het beleid steeds een klein beetje aan te passen op basis van ervaringen uit de omgeving. Hierdoor leert de agent efficiënter en zijn de resultaten vaak beter reproduceerbaar. PPO is ook geschikt voor situaties met hoge-dimensionale input, zoals beelden, en werkt goed in multi-agent omgevingen zoals Warlords.\n",
        "\n",
        "DhanushKumar (2024)\n",
        "\n",
        "### **Motivatie**\n",
        "\n",
        "Voor deze opdracht heb ik gekozen voor het algoritme Proximal Policy Optimization (PPO). De belangrijkste reden hiervoor is dat PPO bekend staat om zijn stabiliteit en efficiëntie bij het trainen van agents in complexe omgevingen met hoge-dimensionale input, zoals de Atari-game Warlords, Schulman et al. (2017). Omdat PPO de aanpassingen aan het beleid per stap beperkt, blijft het leerproces gecontroleerd en voorkom je dat de agent tijdens het trainen “vergeet” wat eerder geleerd is. Dit is vooral belangrijk in multi-agent omgevingen, waar het gedrag van andere agents de situatie voortdurend beïnvloedt.\n",
        "\n",
        "Daarnaast is PPO eenvoudig te implementeren dankzij bestaande libraries zoals stable-baselines3, waardoor het mogelijk is om snel te experimenteren met verschillende hyperparameters. De standaardwaarden die ik voor de belangrijkste hyperparameters heb gekozen, zijn gebaseerd op aanbevelingen uit de literatuur en eerdere succesvolle toepassingen in vergelijkbare omgevingen (The 37 Implementation Details Of Proximal Policy Optimization · The ICLR Blog Track, 2022). PPO is bovendien goed schaalbaar, waardoor het geschikt is om in een multi-agent setting zoals Warlords verschillende agents onafhankelijk van elkaar te trainen en te vergelijken.\n",
        "\n",
        "Door deze eigenschappen is PPO naar mijn mening de meest geschikte keuze voor deze opdracht, omdat het zorgt voor betrouwbare leerresultaten en flexibiliteit biedt bij het uitvoeren van experimenten met verschillende agents en trainingsinstellingen.\n",
        "\n",
        "### **Aanpak**\n",
        "\n",
        "Voor deze opdracht heb ik een PPO-agent geïmplementeerd met behulp van de stable-baselines3 library. PPO is gekozen vanwege de stabiele prestaties en de robuustheid bij het trainen in omgevingen met beeldinput, zoals “Warlords”. De agent gebruikt een convolutioneel neuraal netwerk om de observaties te verwerken.\n",
        "De belangrijkste hyperparameters zijn gekozen op basis van aanbevolen waarden uit wetenschappelijke literatuur voor Atari-omgevingen, maar kunnen verder geoptimaliseerd worden.\n",
        "Mijn implementatie maakt het makkelijk om het model op te slaan, opnieuw te laden en te evalueren, zodat experimenten goed reproduceerbaar zijn."
      ],
      "metadata": {
        "id": "cZW97gbO7eh5"
      },
      "id": "cZW97gbO7eh5"
    },
    {
      "cell_type": "markdown",
      "id": "e9d941ae",
      "metadata": {
        "id": "e9d941ae"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;2.3: Kiezen van Trainingsstrategie</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7196ea",
      "metadata": {
        "id": "5b7196ea"
      },
      "source": [
        "b.\tKies een geschikt RL-algoritme en trainingsstrategie voor je agent. Beschrijf en motiveer je keuzes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Agent 1: (Rogier) DQN - Deep Q-Network </strong>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "05IP74Mw9UG2"
      },
      "id": "05IP74Mw9UG2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainingsstrategie in Multi-Agent Warlords\n",
        "\n",
        "Om optimaal gebruik te maken van het gekozen algoritme, hanteren we een trainingsstrategie die inspeelt op de uitdagingen van multi-agent reinforcement learning:\n",
        "\n",
        "1. **Multi-agent omgeving:**  \n",
        "   In Warlords zijn altijd meerdere agents actief. Onze PPO-agent wordt getraind door herhaaldelijk games te spelen tegen vooraf ingestelde tegenstanders, zoals random agents of (indien gewenst) andere PPO-agents. Dit zorgt ervoor dat de agent leert in een realistische, competitieve setting.\n",
        "\n",
        "2. **Variatie in tegenstanders:**  \n",
        "   Door de agent bloot te stellen aan verschillende typen tegenstanders (van random tot geavanceerd), voorkomen we dat de agent eenzijdige strategieën aanleert. Zo ontwikkelt de agent robuustere, meer generaliseerbare strategieën.\n",
        "\n",
        "3. **Observatie- en actie-preprocessing:**  \n",
        "   We gebruiken beeldverwerkingstechnieken zoals grijswaardenconversie, rescaling en stacking van frames. Dit zorgt ervoor dat de agent alleen de essentiële informatie uit het spel verwerkt, waardoor het leerproces efficiënter verloopt.\n",
        "\n",
        "4. **On-policy leren & rollouts:**  \n",
        "   PPO werkt met zogenoemde rollouts: de agent verzamelt verse trajecten door de omgeving en leert direct van deze actuele ervaringen. Dit betekent dat elke policy-update gebaseerd is op recent gedrag, wat bijdraagt aan stabiliteit en effectieve policy-veranderingen.\n",
        "\n",
        "5. **Regelmatige evaluatie en hyperparameter tuning:**  \n",
        "   Tijdens de training evalueren we regelmatig de prestaties van de agent door deze te laten spelen tegen de baseline (zoals een random agent). Daarnaast experimenteren we met verschillende hyperparameters, zoals learning rate, batch size, en de clipping-range van PPO, om de optimale instellingen te bepalen.\n",
        "\n",
        "6. **Logging en visualisatie:**  \n",
        "   Alle trainingsresultaten (zoals reward curves, winpercentages, etc.) worden gelogd en gevisualiseerd. Hierdoor kunnen we het leerproces volgen en analyseren, en waar nodig de strategie bijstellen.\n",
        "\n",
        "### Samenvattend\n",
        "\n",
        "Deze aanpak zorgt ervoor dat onze PPO-agent niet alleen leert van het eigen gedrag, maar zich ook kan aanpassen aan verschillende soorten tegenstanders. Door systematisch te trainen, evalueren en hyperparameters te tunen, halen we het maximale uit ons algoritme en maken we de voordelen van reinforcement learning in een multi-agent context helder zichtbaar.\n"
      ],
      "metadata": {
        "id": "QdDPbh7p9UNL"
      },
      "id": "QdDPbh7p9UNL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Agent 2: (Julia) PPO - Proximal Policy Optimization (Hier mist denk ik nog wat of ik heb het niet goed doorgelezen)(Julia morgen)</strong>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "OnqBjKqn7uln"
      },
      "id": "OnqBjKqn7uln"
    },
    {
      "cell_type": "markdown",
      "id": "6107d561",
      "metadata": {
        "id": "6107d561"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H3: Probleemdefinitie </strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcbb909",
      "metadata": {
        "id": "6bcbb909"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;3.1: Wat is het probleem?</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d9d8c7",
      "metadata": {
        "id": "31d9d8c7"
      },
      "source": [
        "c.\tResultaat: Een helder uitgewerkte motivatie in je rapport (“Inleiding & Probleemanalyse”)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probleemanalyse\n",
        "De opkomst van multi-agent omgevingen in toepassingen zoals robotica, games en logistiek vraagt om slimme algoritmes die kunnen concurreren én samenwerken. In de praktijk betekent dit dat agents hun strategieën continu moeten bijstellen op basis van het gedrag van andere agents in hun omgeving. In de Atari-game Warlords komen vier agents tegelijkertijd in actie, waarbij hun succes afhankelijk is van zowel hun eigen keuzes als die van hun tegenstanders.\n",
        "\n",
        "Single-agent reinforcement learning is onvoldoende, omdat hierbij wordt aangenomen dat de omgeving stationair is (niet verandert door anderen). In multi-agent settings verandert de omgeving echter continu, omdat andere agents ook leren en hun gedrag aanpassen. Dit vraagt om een benadering waarbij agents niet alleen leren van hun eigen ervaringen, maar ook van de interacties met anderen.\n",
        "\n",
        "Met multi-agent reinforcement learning (MARL) kunnen agents hun beleid optimaliseren terwijl ze rekening houden met de strategieën van anderen. Hierdoor ontstaan vaak complexe en onverwachte gedragingen die in single-agent settings niet mogelijk zijn. Bovendien kunnen MARL-methoden worden ingezet om situaties te modelleren waarin competitie, samenwerking of beide tegelijk nodig zijn.\n",
        "\n",
        "##### Relevantie van het probleem\n",
        "- In veel echte omgevingen zijn meerdere autonome beslissers actief (bijvoorbeeld zelfrijdende auto's in verkeer).\n",
        "- Het ontwerpen van robuuste agents in zulke settings helpt bij het ontwikkelen van realistische, schaalbare en adaptieve AI-systemen.\n",
        "- In de context van games als Warlords kan MARL inzichten bieden in hoe intelligente strategieën en tegenstrategieën ontstaan in competitieve settings.\n",
        "\n",
        "##### Samenvatting probleemstelling (één zin):\n",
        "\"Hoe kunnen we effectieve, lerende agents ontwikkelen die optimaal presteren in een competitieve multi-agent omgeving, waarbij rekening wordt gehouden met de voortdurende interactie en dynamiek tussen verschillende agents?\"\n",
        "\n",
        "Scharwächter (2024)\n",
        "\n",
        "#### Doelstelling\n",
        "Het doel van deze opdracht is om een multi-agent reinforcement learning systeem te ontwerpen, implementeren en evalueren voor de Atari Warlords-omgeving. Dit gebeurt door vier verschillende MARL-algoritmes (PPO, MADDPG, [keuze Peet] en [keuze Jort]) te trainen en hun prestaties te vergelijken met een baseline. Het eindresultaat is een reproduceerbaar systeem en een rapport met een diepgaande analyse van de werking en effectiviteit van de gekozen algoritmen."
      ],
      "metadata": {
        "id": "9ydUYqpqrNmw"
      },
      "id": "9ydUYqpqrNmw"
    },
    {
      "cell_type": "markdown",
      "id": "18f470aa",
      "metadata": {
        "id": "18f470aa"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H4: Ontwerp en Implementatie</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086bad36",
      "metadata": {
        "id": "086bad36"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;4.1: Baseline strategie ontwikkelen</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cffc336",
      "metadata": {
        "id": "0cffc336"
      },
      "source": [
        "a.\tOntwikkel een baseline zoals bijvoorbeeld een rule-based policy een random policy of een andere simpele heuristiek.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Baseline: Random Agent</strong>\n",
        "</div>\n",
        "\n",
        "Als startpunt voor het vergelijken van verschillende algoritmes is het belangrijk om een baseline te definiëren. In deze opdracht gebruiken we een random agent als baseline. Dit is een agent die bij elke stap willekeurig een van de mogelijke acties kiest, ongeacht de observatie of situatie in het spel.\n",
        "\n",
        "**Waarom een random agent als baseline?**\n",
        "\n",
        "Een random agent biedt een objectief referentiepunt: het laat zien wat de prestaties zouden zijn zonder enige vorm van intelligentie, strategie of leren. Door de resultaten van geavanceerdere agents (zoals een rule-based agent of een reinforcement learning agent zoals PPO) te vergelijken met deze random agent, kun je duidelijk aantonen of jouw aanpak daadwerkelijk beter presteert dan toeval.\n",
        "\n",
        "**Implementatie**\n",
        "\n",
        "De implementatie van de random agent is heel eenvoudig. De agent kiest telkens een willekeurige actie uit het totale aantal toegestane acties van de omgeving. In het geval van Atari Warlords zijn dit bijvoorbeeld zes mogelijke acties.\n"
      ],
      "metadata": {
        "id": "hmQKYc-6tRob"
      },
      "id": "hmQKYc-6tRob"
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentRandomPolicy:\n",
        "    def act(self, observation):\n",
        "        # Return a random action (6 possible in ALE Warlords)\n",
        "        return np.random.randint(6)\n"
      ],
      "metadata": {
        "id": "IXQkWF0RsqFo"
      },
      "id": "IXQkWF0RsqFo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b41eef89",
      "metadata": {
        "id": "b41eef89"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;4.2: Selectie van DRL algoritme en frameworks</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e23682d",
      "metadata": {
        "id": "6e23682d"
      },
      "source": [
        "b.\tKies een passend DRL-algoritme, die geschikt is voor een multi-agent setting. Kies geschikte packages en frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Dit is uitgewerkt maar niet iedereen zijn agent staat er nu bij sinds ik dat nog niet weet en of je packages etc zijn uitgelegd. Iedereen moet dat hier zelf in toevoegen <strong> </div>\n",
        "\n"
      ],
      "metadata": {
        "id": "4K5_Zj8T-a1N"
      },
      "id": "4K5_Zj8T-a1N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keuze van Algoritme\n",
        "\n",
        "Voor deze opdracht, waarin de agent moet presteren in de **multi-agent omgeving van Atari Warlords**, kiezen we voor het **Proximal Policy Optimization (PPO)** algoritme. PPO is momenteel één van de meest gebruikte en robuuste algoritmes voor deep reinforcement learning, vooral geschikt voor problemen met complexe visuele input en multi-agent interactie. De belangrijkste voordelen van PPO:\n",
        "\n",
        "- **Stabiel leren van ruwe pixels:** PPO maakt gebruik van convolutionele neurale netwerken (CNN’s) om direct van visuele observaties te leren, zonder handmatig feature engineering.\n",
        "- **Uitstekende prestaties in Atari-omgevingen:** PPO heeft zich bewezen als benchmark-algoritme in veel Atari-games, mede dankzij de balans tussen exploratie en exploitatie.\n",
        "- **Direct geschikt voor multi-agent settings:** PPO kan eenvoudig worden ingezet met *parameter sharing* (één policy voor meerdere agents) of individuele policies per agent, wat het flexibel maakt voor uiteenlopende MARL-experimenten.\n",
        "\n",
        "### Keuze & Motivatie\n",
        "\n",
        "In deze opdracht trainen we een **PPO-agent** in de multi-agent Warlords-omgeving. Hierbij nemen onze agenten het op tegen ingebouwde tegenstanders, zoals random agents. Zo kunnen we de kracht van deep RL aantonen in vergelijking met simpele baseline strategieën.\n",
        "\n",
        "#### Waarom PPO in deze context?\n",
        "\n",
        "- **Stabiele policy learning:** PPO minimaliseert het risico op instabiliteit door gecontroleerde policy-updates (clipping), wat vooral belangrijk is in chaotische multi-agent omgevingen.\n",
        "- **Visual input:** Warlords levert observaties als pixeldata, wat naadloos aansluit op de CNN-architectuur van PPO.\n",
        "- **Flexibiliteit:** PPO werkt zowel met discrete als continue actie-ruimtes en laat zich makkelijk combineren met moderne MARL frameworks.\n",
        "\n",
        "---\n",
        "\n",
        "### Packages en Frameworks\n",
        "\n",
        "Voor de implementatie en evaluatie maken we gebruik van de volgende frameworks en libraries:\n",
        "\n",
        "#### 1. **Stable-Baselines3**\n",
        "- Biedt een krachtige en stabiele implementatie van PPO, met uitgebreide ondersteuning voor logging en evaluatie.\n",
        "- Direct compatibel met vectorized en custom omgevingen.\n",
        "\n",
        "#### 2. **PettingZoo**\n",
        "- De standaard voor multi-agent reinforcement learning omgevingen, met een ruime keuze aan benchmarkomgevingen zoals Atari Warlords.\n",
        "- Zorgt voor een uniforme interface en maakt snelle experimentatie mogelijk.\n",
        "\n",
        "#### 3. **Supersuit**\n",
        "- Bibliotheek voor preprocessing (o.a. frames resizen, kleuren reduceren en frame stacking), essentieel voor efficiënte training op visuele data.\n",
        "\n",
        "---\n",
        "\n",
        "### Samenvatting van de strategie\n",
        "\n",
        "- We trainen een PPO-agent in de **PettingZoo Atari Warlords** omgeving tegen baseline agents (zoals random).\n",
        "- PPO stelt ons in staat om efficiënt en stabiel te leren in deze multi-agent setting, met als doel beter te presteren dan eenvoudige baselines.\n",
        "- Door te bouwen op bewezen frameworks en preprocessing pipelines is onze aanpak **reproduceerbaar, schaalbaar en robuust**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sm8SLNH6wAbI"
      },
      "id": "sm8SLNH6wAbI"
    },
    {
      "cell_type": "markdown",
      "id": "e51ff2f0",
      "metadata": {
        "id": "e51ff2f0"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;4.3: Implementatie MARL-agent</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327a104c",
      "metadata": {
        "id": "327a104c"
      },
      "source": [
        "d.\tResultaat: Een werkend MARL-systeem dat klaar is voor training en evaluatie."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>(Rogier) PPO Agent ( DEZE FUNCTIE NOG ZETTEN BIJ DE TRAINING STAP )</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "3us4a2TXs3IY"
      },
      "id": "3us4a2TXs3IY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In de onderstaande cel definiëren we de klasse MARLAgentPPO, een modulair systeem dat geschikt is voor het trainen van PPO-agenten in een multi-agent setting met behulp van de PettingZoo-omgeving warlords_v3. De klasse bevat methoden voor het instellen van de omgeving, het trainen van het model, en het opslaan of laden van een getraind PPO-model.\n",
        "\n",
        " -   De omgeving wordt gepreprocessed met behulp van Supersuit-wrappers (zoals black_death, color_reduction, resizing en frame stacking).\n",
        "\n",
        "-  Vervolgens wordt de omgeving geconverteerd naar een vectorized environment die compatibel is met Stable-Baselines3.\n",
        "\n",
        "-   De train()-methode traint het PPO-model met een CnnPolicy, geschikt voor de beeldinvoer van Atari-games.\n",
        "\n",
        "-  Met save() en load() kunnen getrainde modellen eenvoudig worden opgeslagen of ingeladen.\n",
        "\n",
        "Deze klasse maakt het mogelijk om met ons MARL-systeem op een reproduceerbare en schaalbare manier PPO-agenten te trainen in een multi-agent omgeving.\n",
        "\n",
        "(PPO — Stable Baselines3 2.7.0a0 Documentation, z.d.)"
      ],
      "metadata": {
        "id": "QVf1ym4iZO3C"
      },
      "id": "QVf1ym4iZO3C"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy\n",
        "from pettingzoo.atari import warlords_v3\n",
        "\n",
        "class MARLAgentPPO:\n",
        "    \"\"\"\n",
        "    Multi-Agent RL systeem voor de Atari Warlords omgeving.\n",
        "    Ondersteunt setup, training, evaluatie en het opslaan/laden van PPO-modellen.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_vec_envs=2,\n",
        "        frame_stack=4,\n",
        "        x_size=84,\n",
        "        y_size=84,\n",
        "        batch_size=256,\n",
        "        total_timesteps=1_000_000,\n",
        "        verbose=3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialiseer de omgeving en hyperparameters.\n",
        "        \"\"\"\n",
        "        self.num_vec_envs = num_vec_envs\n",
        "        self.frame_stack = frame_stack\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.batch_size = batch_size\n",
        "        self.total_timesteps = total_timesteps\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.env = self._make_env()\n",
        "        self.vec_env = self._make_vec_env()\n",
        "        self.model = None\n",
        "\n",
        "    def _make_env(self):\n",
        "        \"\"\"\n",
        "        Zet de Warlords-omgeving klaar met preprocessing-wrappers.\n",
        "        \"\"\"\n",
        "        env = warlords_v3.parallel_env()\n",
        "        env = ss.black_death_v3(env)\n",
        "        env = ss.color_reduction_v0(env, mode='full')\n",
        "        env = ss.resize_v1(env, x_size=self.x_size, y_size=self.y_size)\n",
        "        env = ss.frame_stack_v1(env, self.frame_stack)\n",
        "        return env\n",
        "\n",
        "    def _make_vec_env(self):\n",
        "        \"\"\"\n",
        "        Converteer naar een vectorized environment voor Stable-Baselines3.\n",
        "        \"\"\"\n",
        "        vec_env = ss.pettingzoo_env_to_vec_env_v1(self.env)\n",
        "        vec_env = ss.concat_vec_envs_v1(\n",
        "            vec_env,\n",
        "            num_vec_envs=self.num_vec_envs,\n",
        "            num_cpus=1,\n",
        "            base_class=\"stable_baselines3\"\n",
        "        )\n",
        "        return vec_env\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Initialiseer en train het PPO-model.\n",
        "        \"\"\"\n",
        "        print(\"Start training...\")\n",
        "        self.model = PPO(\n",
        "            CnnPolicy,\n",
        "            self.vec_env,\n",
        "            verbose=self.verbose,\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "        self.model.learn(total_timesteps=self.total_timesteps)\n",
        "        print(\"Training gereed.\")\n",
        "\n",
        "    def save(self, model_name=None):\n",
        "        \"\"\"\n",
        "        Sla het getrainde model op, optioneel met custom naam.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model is nog niet getraind.\")\n",
        "        if model_name is None:\n",
        "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "            model_name = f\"warlords_ppo_model_{timestamp}\"\n",
        "        self.model.save(model_name)\n",
        "        print(f\"Model opgeslagen als {model_name}\")\n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\"\n",
        "        Laad een eerder getraind PPO-model.\n",
        "        \"\"\"\n",
        "        self.model = PPO.load(path)\n",
        "        print(f\"Model geladen van {path}\")\n"
      ],
      "metadata": {
        "id": "hz_u0D-ls5au"
      },
      "id": "hz_u0D-ls5au",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MARLAgentPPO testen\n",
        "\n",
        "agent = MARLAgentPPO()\n",
        "agent.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYsbyJNJbm-R",
        "outputId": "b1fd1436-fb78-4944-9d77-efd9b12146dd"
      },
      "id": "jYsbyJNJbm-R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
            "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 686   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 23    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 535         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 61          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008296274 |\n",
            "|    clip_fraction        | 0.0256      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.79       |\n",
            "|    explained_variance   | -3.35       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0231     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 0.000737    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 503         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 97          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010379068 |\n",
            "|    clip_fraction        | 0.0971      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | 0.156       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0238     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00835    |\n",
            "|    value_loss           | 0.00158     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 490         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 133         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006755204 |\n",
            "|    clip_fraction        | 0.0646      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | 0.0799      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0205     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0104     |\n",
            "|    value_loss           | 0.00267     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 486         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 168         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011947133 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | -0.0601     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0169     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0129     |\n",
            "|    value_loss           | 0.000623    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 484         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 202         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012139898 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | 0.0229      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0351     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    value_loss           | 0.000597    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 483         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 237         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009495772 |\n",
            "|    clip_fraction        | 0.0974      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | 0.0688      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0142     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00952    |\n",
            "|    value_loss           | 0.00201     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 482         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 271         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010106129 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | -0.0773     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0439     |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 0.00053     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 480          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 306          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0095645115 |\n",
            "|    clip_fraction        | 0.105        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.75        |\n",
            "|    explained_variance   | 0.0062       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.0134      |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.0117      |\n",
            "|    value_loss           | 0.00265      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 341         |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012384109 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | 0.0228      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0269     |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    value_loss           | 0.00103     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 375         |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010372435 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | -0.0121     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0236     |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0128     |\n",
            "|    value_loss           | 0.00205     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 480          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 409          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0129157575 |\n",
            "|    clip_fraction        | 0.173        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.74        |\n",
            "|    explained_variance   | -0.0669      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | -0.0259      |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.0181      |\n",
            "|    value_loss           | 0.00163      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 442         |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011168885 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.75       |\n",
            "|    explained_variance   | -0.0251     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0254     |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    value_loss           | 0.000524    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 478         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008375325 |\n",
            "|    clip_fraction        | 0.0956      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | -0.0245     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0231     |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 0.000967    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 512         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011947181 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | -0.0437     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0118     |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 0.00265     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 547         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016368235 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | 0.0362      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0534     |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0387     |\n",
            "|    value_loss           | 2.63e-05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 580         |\n",
            "|    total_timesteps      | 278528      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010357081 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -0.0018     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0332     |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0152     |\n",
            "|    value_loss           | 0.00101     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 615         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012391489 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -0.0191     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0281     |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 0.00261     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 479        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 649        |\n",
            "|    total_timesteps      | 311296     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02064908 |\n",
            "|    clip_fraction        | 0.223      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.71      |\n",
            "|    explained_variance   | -1.84      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0689    |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0462    |\n",
            "|    value_loss           | 2.53e-05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 683         |\n",
            "|    total_timesteps      | 327680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014955677 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | -0.012      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0261     |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    value_loss           | 0.00151     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 718         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014626913 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | -0.048      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0334     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    value_loss           | 0.00161     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 751         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019847143 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | -0.0366     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0011     |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0251     |\n",
            "|    value_loss           | 0.00108     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 785         |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022878055 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.62       |\n",
            "|    explained_variance   | -0.00454    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0674     |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0293     |\n",
            "|    value_loss           | 0.000499    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 819         |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020858679 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | -0.0688     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0261     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0352     |\n",
            "|    value_loss           | 0.000523    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 479        |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 854        |\n",
            "|    total_timesteps      | 409600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01582524 |\n",
            "|    clip_fraction        | 0.184      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.66      |\n",
            "|    explained_variance   | -0.00601   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0352    |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.0198    |\n",
            "|    value_loss           | 0.00157    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 26         |\n",
            "|    time_elapsed         | 887        |\n",
            "|    total_timesteps      | 425984     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01731611 |\n",
            "|    clip_fraction        | 0.192      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.66      |\n",
            "|    explained_variance   | -0.00529   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0458    |\n",
            "|    n_updates            | 250        |\n",
            "|    policy_gradient_loss | -0.0208    |\n",
            "|    value_loss           | 0.00156    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 921         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017137889 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0.00097     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0335     |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 0.002       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 955         |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021975514 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.69       |\n",
            "|    explained_variance   | -1.19       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0777     |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0566     |\n",
            "|    value_loss           | 2.24e-05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 989         |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015616686 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | -0.00346    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0402     |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    value_loss           | 0.00316     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 1027        |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025378067 |\n",
            "|    clip_fraction        | 0.265       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.62       |\n",
            "|    explained_variance   | -0.0257     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0293     |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0407     |\n",
            "|    value_loss           | 0.000526    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 1061        |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017625881 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0.0406      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0467     |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0298     |\n",
            "|    value_loss           | 0.000491    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 1096        |\n",
            "|    total_timesteps      | 524288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018755853 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.67       |\n",
            "|    explained_variance   | -0.00472    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0305     |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0241     |\n",
            "|    value_loss           | 0.00109     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 478        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 1129       |\n",
            "|    total_timesteps      | 540672     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02615742 |\n",
            "|    clip_fraction        | 0.286      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.59      |\n",
            "|    explained_variance   | -0.0148    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0661    |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.0258    |\n",
            "|    value_loss           | 0.00108    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 1164        |\n",
            "|    total_timesteps      | 557056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020254305 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | -0.00296    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0217     |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    value_loss           | 0.00202     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 1199        |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027309649 |\n",
            "|    clip_fraction        | 0.289       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | -0.031      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0803     |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.0339     |\n",
            "|    value_loss           | 0.00103     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 1233        |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023422813 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | -0.018      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0793     |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0321     |\n",
            "|    value_loss           | 0.00101     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 1268        |\n",
            "|    total_timesteps      | 606208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023118062 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | -0.0224     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0227     |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    value_loss           | 0.00216     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 1301        |\n",
            "|    total_timesteps      | 622592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025609674 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.58       |\n",
            "|    explained_variance   | -0.00419    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0159     |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0253     |\n",
            "|    value_loss           | 0.00098     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 1335        |\n",
            "|    total_timesteps      | 638976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021689672 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | -0.0257     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0231     |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    value_loss           | 0.00205     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 478        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 1368       |\n",
            "|    total_timesteps      | 655360     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02426009 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.6       |\n",
            "|    explained_variance   | -0.0509    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0256    |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | -0.0287    |\n",
            "|    value_loss           | 0.00159    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 478        |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 1402       |\n",
            "|    total_timesteps      | 671744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03276883 |\n",
            "|    clip_fraction        | 0.314      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.57      |\n",
            "|    explained_variance   | -3.18      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0886    |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | -0.0612    |\n",
            "|    value_loss           | 1.42e-05   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 1435        |\n",
            "|    total_timesteps      | 688128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018674042 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | -0.00231    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.023      |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    value_loss           | 0.000968    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 1469        |\n",
            "|    total_timesteps      | 704512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019024104 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | -0.0413     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0312     |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    value_loss           | 0.00318     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 1502        |\n",
            "|    total_timesteps      | 720896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028248962 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.58       |\n",
            "|    explained_variance   | 0.00425     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0513     |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0356     |\n",
            "|    value_loss           | 0.001       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 1536        |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029916143 |\n",
            "|    clip_fraction        | 0.28        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | -0.0634     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0305     |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0369     |\n",
            "|    value_loss           | 0.00109     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 1569        |\n",
            "|    total_timesteps      | 753664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036699045 |\n",
            "|    clip_fraction        | 0.344       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | -0.36       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.109      |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0691     |\n",
            "|    value_loss           | 4.68e-06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 1604        |\n",
            "|    total_timesteps      | 770048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018005997 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.55       |\n",
            "|    explained_variance   | 0.00365     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0163     |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    value_loss           | 0.00202     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 1637        |\n",
            "|    total_timesteps      | 786432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028369252 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | -0.0212     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0264     |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.029      |\n",
            "|    value_loss           | 0.00209     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 1671       |\n",
            "|    total_timesteps      | 802816     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04052819 |\n",
            "|    clip_fraction        | 0.373      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.49      |\n",
            "|    explained_variance   | -3.18      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.102     |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.0682    |\n",
            "|    value_loss           | 2.15e-05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 50         |\n",
            "|    time_elapsed         | 1704       |\n",
            "|    total_timesteps      | 819200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01972577 |\n",
            "|    clip_fraction        | 0.182      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.53      |\n",
            "|    explained_variance   | 0.00149    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.015     |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.0156    |\n",
            "|    value_loss           | 0.00208    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 51         |\n",
            "|    time_elapsed         | 1738       |\n",
            "|    total_timesteps      | 835584     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03362392 |\n",
            "|    clip_fraction        | 0.302      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.55      |\n",
            "|    explained_variance   | -0.0789    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0867    |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | -0.0484    |\n",
            "|    value_loss           | 0.000527   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1772        |\n",
            "|    total_timesteps      | 851968      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028167294 |\n",
            "|    clip_fraction        | 0.266       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | -0.0136     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0384     |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0253     |\n",
            "|    value_loss           | 0.00157     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 1806       |\n",
            "|    total_timesteps      | 868352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02409561 |\n",
            "|    clip_fraction        | 0.214      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.59      |\n",
            "|    explained_variance   | -0.00132   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.016     |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | -0.0206    |\n",
            "|    value_loss           | 0.00209    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 1840        |\n",
            "|    total_timesteps      | 884736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030931419 |\n",
            "|    clip_fraction        | 0.287       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | -0.0235     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0032     |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0231     |\n",
            "|    value_loss           | 0.0016      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1874        |\n",
            "|    total_timesteps      | 901120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044653084 |\n",
            "|    clip_fraction        | 0.437       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | -7.27       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.124      |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.0843     |\n",
            "|    value_loss           | 9.58e-06    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1909        |\n",
            "|    total_timesteps      | 917504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023714889 |\n",
            "|    clip_fraction        | 0.239       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 7.75e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0221     |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 0.00205     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1942        |\n",
            "|    total_timesteps      | 933888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023891259 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.56       |\n",
            "|    explained_variance   | -0.0271     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0342     |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0246     |\n",
            "|    value_loss           | 0.00267     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 480        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 1976       |\n",
            "|    total_timesteps      | 950272     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03417486 |\n",
            "|    clip_fraction        | 0.299      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.54      |\n",
            "|    explained_variance   | -0.0183    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0252    |\n",
            "|    n_updates            | 570        |\n",
            "|    policy_gradient_loss | -0.034     |\n",
            "|    value_loss           | 0.00103    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 2010        |\n",
            "|    total_timesteps      | 966656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030938981 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.58       |\n",
            "|    explained_variance   | -0.0224     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0905     |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.029      |\n",
            "|    value_loss           | 0.00108     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 480         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 2044        |\n",
            "|    total_timesteps      | 983040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028373862 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | -0.00445    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.027      |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    value_loss           | 0.00159     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 481         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 2077        |\n",
            "|    total_timesteps      | 999424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043455176 |\n",
            "|    clip_fraction        | 0.373       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | -0.0384     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.101      |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.0507     |\n",
            "|    value_loss           | 0.000501    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 481         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 2110        |\n",
            "|    total_timesteps      | 1015808     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033999108 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.55       |\n",
            "|    explained_variance   | -0.025      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0723     |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.031      |\n",
            "|    value_loss           | 0.00127     |\n",
            "-----------------------------------------\n",
            "Training gereed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>(Julia) PPO Agent (WEET NIET OF DEZE OOK OPRECHT WERKT MET TRAINING)</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "cNjPE8PgvRA-"
      },
      "id": "cNjPE8PgvRA-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Getrainde agent opslaan\n",
        "agent.save(\"ppo_model_warlords\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKJCURytnf7g",
        "outputId": "9b4158dc-6144-443f-8e7b-78a551bb49a4"
      },
      "id": "iKJCURytnf7g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model opgeslagen als ppo_model_warlords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/MARL_models\""
      ],
      "metadata": {
        "id": "Tc4nkwHvqDZi"
      },
      "id": "Tc4nkwHvqDZi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Zorg dat de map bestaat\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Sla model op met een custom naam en pad\n",
        "model_name = os.path.join(save_path, \"warlords_ppo_model\")\n",
        "agent.save(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nYFI8LsqF1G",
        "outputId": "067f412f-cd67-4ab5-877f-e87cd35fec68"
      },
      "id": "0nYFI8LsqF1G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model opgeslagen als /content/drive/MyDrive/MARL_models/warlords_ppo_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.load(\"ppo_model_warlords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ProZJWxRpZuo",
        "outputId": "0ffc531e-c3b1-4ec0-a03d-d40ad18729a9"
      },
      "id": "ProZJWxRpZuo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model geladen van ppo_model_warlords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/MARL_models/warlords_ppo_model.zip\"\n",
        "agent.load(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YujIDpuSqzCs",
        "outputId": "fc5f939d-264b-4d9c-955e-3697fe4771b4"
      },
      "id": "YujIDpuSqzCs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model geladen van /content/drive/MyDrive/MARL_models/warlords_ppo_model.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent:\n",
        "    \"\"\"\n",
        "    Proximal Policy Optimization (PPO) agent for the Atari Warlords environment.\n",
        "\n",
        "    Attributes:\n",
        "        env_name (str): Name of the gym environment.\n",
        "        env: The instantiated gym environment.\n",
        "        model: The PPO model from stable-baselines3.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_name=\"ALE/Warlords-v5\"):\n",
        "        \"\"\"\n",
        "        Initializes the PPOAgent with a specified environment.\n",
        "\n",
        "        Parameters:\n",
        "            env_name (str): Gym environment name. Defaults to \"ALE/Warlords-v5\".\n",
        "        \"\"\"\n",
        "        self.env = gym.make(env_name)\n",
        "        # CnnPolicy, want de input zijn afbeeldingen\n",
        "        self.model = PPO(\n",
        "            \"CnnPolicy\",\n",
        "            self.env,\n",
        "            verbose=1,\n",
        "            learning_rate=2.5e-4,\n",
        "            gamma=0.99,\n",
        "            n_steps=128,\n",
        "            batch_size=256,\n",
        "            ent_coef=0.01\n",
        "        )\n",
        "        self.env_name = env_name\n",
        "\n",
        "    def train(self, timesteps=100_000):\n",
        "        \"\"\"\n",
        "        Train the PPO model for a specified number of timesteps.\n",
        "\n",
        "        Parameters:\n",
        "            timesteps (int): Number of training timesteps.\n",
        "        \"\"\"\n",
        "        self.model.learn(total_timesteps=timesteps)\n",
        "\n",
        "    def act(self, observation):\n",
        "        \"\"\"\n",
        "        Selects an action based on the current observation using the trained model.\n",
        "\n",
        "        Parameters:\n",
        "            observation (np.ndarray): Current environment observation.\n",
        "\n",
        "        Returns:\n",
        "            int: Chosen action.\n",
        "        \"\"\"\n",
        "        action, _ = self.model.predict(observation, deterministic=True)\n",
        "        return action\n",
        "\n",
        "    def save(self, path=\"ppo_warlords.zip\"):\n",
        "        \"\"\"\n",
        "        Saves the trained model to a file.\n",
        "\n",
        "        Parameters:\n",
        "            path (str): Path to save the model.\n",
        "        \"\"\"\n",
        "        self.model.save(path)\n",
        "\n",
        "    def load(self, path=\"ppo_warlords.zip\"):\n",
        "        \"\"\"\n",
        "        Loads a trained model from a file.\n",
        "\n",
        "        Parameters:\n",
        "            path (str): Path to the saved model.\n",
        "        \"\"\"\n",
        "        self.model = PPO.load(path, env=self.env)\n",
        "\n",
        "    def evaluate(self, episodes=5):\n",
        "        \"\"\"\n",
        "        Evaluates the PPO agent for a given number of episodes and prints the total reward.\n",
        "\n",
        "        Parameters:\n",
        "            episodes (int): Number of evaluation episodes.\n",
        "        \"\"\"\n",
        "        env = gym.make(self.env_name)\n",
        "        for ep in range(episodes):\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            while not done:\n",
        "                action = self.act(obs)\n",
        "                obs, reward, done, info = env.step(action)\n",
        "                total_reward += reward\n",
        "                env.render()\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
        "        env.close()"
      ],
      "metadata": {
        "id": "HGPCukzcuwOZ"
      },
      "id": "HGPCukzcuwOZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "210914e9",
      "metadata": {
        "id": "210914e9"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H5: Training en Hyperparameter Search</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0e45d6",
      "metadata": {
        "id": "1c0e45d6"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;5.1: Training</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>(Rogier) Agent 1: PPO - Proximal Policy Optimization</strong></div>"
      ],
      "metadata": {
        "id": "RhfhmR3x-9DU"
      },
      "id": "RhfhmR3x-9DU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hoe verloopt de training van een PPO-agent?\n",
        "\n",
        "De training van een PPO-agent (Proximal Policy Optimization) in een omgeving zoals Atari Warlords bestaat uit een reeks vaste stappen die gericht zijn op stabiliteit en efficiënt leren. Tijdens elke trainingscyclus verzamelt de agent zogenaamde 'rollouts': sequenties van observaties, acties en rewards die worden gegenereerd door het huidige beleid. Op basis van deze trajecten berekent PPO met Generalized Advantage Estimation (GAE) hoe gunstig elke actie was ten opzichte van de verwachting, wat helpt om het leerproces stabieler en nauwkeuriger te maken.\n",
        "\n",
        "De kern van PPO ligt in het voorzichtig updaten van het beleid. In plaats van grote stappen (die tot instabiliteit kunnen leiden), worden policy-updates beperkt door een clipping-mechanisme. Dit voorkomt dat het geleerde beleid te veel verandert per trainingsstap, waardoor de kans op catastrofaal “vergeten” sterk wordt verminderd. De verzamelde rollouts worden verdeeld over mini-batches en gedurende meerdere epochs gebruikt om zowel het beleid (policy) als de value-functie (critic) te verbeteren. Tegelijkertijd stimuleert PPO via een entropiebonus dat de agent in het begin veel blijft exploreren, wat de kans op het vinden van sterke strategieën vergroot.\n",
        "\n",
        "Tijdens en na de training wordt de agent regelmatig geëvalueerd zonder extra ruis, zodat duidelijk wordt hoe effectief het geleerde beleid is in de praktijk. Door deze aanpak is PPO bijzonder geschikt voor complexe, visuele en multi-agent omgevingen, waarbij gecontroleerd leren en robuuste prestaties essentieel zijn.\n"
      ],
      "metadata": {
        "id": "vO98uFAF_FCz"
      },
      "id": "vO98uFAF_FCz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Omgeving opzetten</strong>\n",
        "</div>\n",
        "\n",
        "\n",
        "In deze stap initialiseren we de multi-agent Warlords-omgeving met behulp van PettingZoo. We passen verschillende bewerkingen toe op de raw game frames:\n",
        "- `warlords_v3.parallel_env()`: Laadt de 4-speler Warlords-omgeving waarbij alle agents gelijktijdig acties nemen.\n",
        "- `ss.black_death_v3(env)`: Zorgt ervoor dat agents die \"dood\" gaan toch in de omgeving blijven als placeholder, zodat de agent-count altijd gelijk blijft.\n",
        "- `ss.color_reduction_v0(env, mode='full')`: Zet de gekleurde frames om naar grijswaarden, waardoor de inputdimensie kleiner wordt en het leren efficiënter.\n",
        "- `ss.resize_v1(env, x_size=84, y_size=84)`: Schaal de beelden terug naar 84x84 pixels (standaard in Atari-RL-onderzoek).\n",
        "- `ss.frame_stack_v1(env, 4)`: Stapelt de laatste 4 frames, zodat het model ook bewegingsinformatie kan oppikken (belangrijk bij visuele input).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "W5whiJBiBH11"
      },
      "id": "W5whiJBiBH11"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Maak de parallelle multi-agent omgeving\n",
        "env = warlords_v3.parallel_env()            # 4-player Warlords omgeving\n",
        "env = ss.black_death_v3(env)               # Houd agents in env, zelfs na \"dood\"\n",
        "env = ss.color_reduction_v0(env, mode='full')  # Converteer naar grijswaarden:contentReference[oaicite:22]{index=22}\n",
        "env = ss.resize_v1(env, x_size=84, y_size=84)   # Resize frames naar 84x84 pixels\n",
        "env = ss.frame_stack_v1(env, 4)                # Stack 4 opvolgende frames:contentReference[oaicite:23]{index=23}"
      ],
      "metadata": {
        "id": "bm8m3LGworU1"
      },
      "id": "bm8m3LGworU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Converteren naar een vectorized env voor Stable-Baselines3</strong>\n",
        "</div>\n",
        "\n",
        "Stable-Baselines3 werkt het best met vectorized environments, waarbij meerdere instanties van de omgeving parallel kunnen draaien:\n",
        "- `ss.pettingzoo_env_to_vec_env_v1(env)`: Zet de PettingZoo-omgeving om naar een formaat dat door SB3 wordt herkend.\n",
        "- `ss.concat_vec_envs_v1(...)`: Combineert meerdere vectorized omgevingen tot één, zodat batchgewijze training mogelijk is (hier draaien 2 parallelle omgevingen op 1 CPU).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5sisIiBjE1ji"
      },
      "id": "5sisIiBjE1ji"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Converteer naar een vectorized env voor Stable-Baselines3\n",
        "vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "vec_env = ss.concat_vec_envs_v1(vec_env, num_vec_envs=2, num_cpus=1, base_class=\"stable_baselines3\")"
      ],
      "metadata": {
        "id": "SNKqddWBotHw"
      },
      "id": "SNKqddWBotHw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Defineren en trainen van het PPO model</strong>\n",
        "</div>\n",
        "\n",
        "We definiëren het PPO-model dat gebruikmaakt van een convolutioneel neuraal netwerk (CnnPolicy), geschikt voor visuele input zoals Atari-frames:\n",
        "- `PPO(CnnPolicy, vec_env, ...)`: Initialiseer het PPO-algoritme met de eerder gemaakte vectorized environment.\n",
        "- `total_timesteps`: Bepaalt hoe lang het model traint. Meer timesteps betekent meestal beter getrainde agenten.\n",
        "- `model.learn(...)`: Start het daadwerkelijke leerproces. Het model verzamelt data, leert van ervaringen en past het beleid continu aan.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vLtNh7tEozmE"
      },
      "id": "vLtNh7tEozmE"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definieer en train het PPO model\n",
        "model = PPO(CnnPolicy, vec_env, verbose=3, batch_size=256)  # CNN-beleid voor visuele input:contentReference[oaicite:24]{index=24}\n",
        "total_timesteps = 10_000_000   # kies het aantal trainingstimesteps\n",
        "print(\"Start training...\")\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "print(\"Training gereed.\")"
      ],
      "metadata": {
        "id": "rnsPATHcozu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77823eb5-fdbc-446e-cd49-2bb76654d325"
      },
      "id": "rnsPATHcozu8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
            "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 685   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 23    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 557         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 58          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007993929 |\n",
            "|    clip_fraction        | 0.0178      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.79       |\n",
            "|    explained_variance   | -1.78       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00604     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    value_loss           | 0.00127     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 525         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 93          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009628351 |\n",
            "|    clip_fraction        | 0.0651      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | 0.272       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0145     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00841    |\n",
            "|    value_loss           | 0.000545    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 517         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 126         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008323721 |\n",
            "|    clip_fraction        | 0.0643      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | 0.178       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0141     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00845    |\n",
            "|    value_loss           | 0.00201     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 510         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 160         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009123579 |\n",
            "|    clip_fraction        | 0.0985      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | 0.064       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.000924    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00882    |\n",
            "|    value_loss           | 0.00163     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 507         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 193         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009636432 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.0798      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0215     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    value_loss           | 0.00246     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 505         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 227         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010353488 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.77       |\n",
            "|    explained_variance   | -0.0259     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0298     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    value_loss           | 0.00114     |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/lambda_wrappers/observation_lambda.py\u001b[0m in \u001b[0;36m_modify_observation\u001b[0;34m(self, agent, observation)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_observation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_obs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: basic_obs_wrapper.<locals>.change_obs() takes 2 positional arguments but 3 were given",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-4260403329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10_000_000\u001b[0m   \u001b[0;31m# kies het aantal trainingstimesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training gereed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Transpose the terminal observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/vector/sb3_vector_wrapper.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Note: SB3 expects dones to be an np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         dones = np.array(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/vector/concat_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/vector/concat_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvenv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             data.append(\n\u001b[0;32m---> 84\u001b[0;31m                 venv.step(\n\u001b[0m\u001b[1;32m     85\u001b[0m                     self.concatenate_actions(\n\u001b[1;32m     86\u001b[0m                         \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/vector/markov_vector_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         }\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpar_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# adds last observation to info where user can get it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/generic_wrappers/utils/shared_wrapper_util.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         }\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_modifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         observations = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/wrappers/base_parallel.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAgentID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     ]:\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/conversions.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mtruncations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         observations = {\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/conversions.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         observations = {\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         }\n\u001b[1;32m    217\u001b[0m         while self.aec_env.agents and (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/utils/base_aec_wrapper.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         obs = super().observe(\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )  # problem is in this line, the obs is sometimes a different size from the obs space\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mEnvLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_observe_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mObsType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/utils/base_aec_wrapper.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )  # problem is in this line, the obs is sometimes a different size from the obs space\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modify_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/lambda_wrappers/observation_lambda.py\u001b[0m in \u001b[0;36m_modify_observation\u001b[0;34m(self, agent, observation)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_observation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_obs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_observation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_obs_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/generic_wrappers/basic_wrappers.py\u001b[0m in \u001b[0;36mchange_obs\u001b[0;34m(obs, obs_space)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchange_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_lambda_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/supersuit/utils/basic_transforms/color_reduction.py\u001b[0m in \u001b[0;36mchange_observation\u001b[0;34m(obs, obs_space, color_reduction)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_reduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mGRAYSCALE_WEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Opslaan van het getrainde model met timestamp</strong>\n",
        "</div>\n",
        "\n",
        "Na de training slaan we het model op, waarbij automatisch een timestamp aan de bestandsnaam wordt toegevoegd:\n",
        "- `time.strftime(...)`: Maakt een string van de huidige datum en tijd.\n",
        "- `model.save(model_name)`: Slaat het getrainde model op onder een unieke naam, zodat je het later gemakkelijk kunt laden en evalueren.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4-c6Bba0EduX"
      },
      "id": "4-c6Bba0EduX"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Sla het getrainde model op\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_name = f\"warlords_ppo_model_{timestamp}\"\n",
        "model.save(model_name)\n",
        "print(f\"Model opgeslagen als {model_name}\")"
      ],
      "metadata": {
        "id": "yrTYXTf_pM8M"
      },
      "id": "yrTYXTf_pM8M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy\n",
        "from pettingzoo.atari import warlords_v3\n",
        "\n",
        "# 1. Maak de parallelle multi-agent omgeving\n",
        "env = warlords_v3.parallel_env()            # 4-player Warlords omgeving\n",
        "env = ss.black_death_v3(env)               # Houd agents in env, zelfs na \"dood\"\n",
        "env = ss.color_reduction_v0(env, mode='full')  # Converteer naar grijswaarden:contentReference[oaicite:22]{index=22}\n",
        "env = ss.resize_v1(env, x_size=84, y_size=84)   # Resize frames naar 84x84 pixels\n",
        "env = ss.frame_stack_v1(env, 4)                # Stack 4 opvolgende frames:contentReference[oaicite:23]{index=23}\n",
        "\n",
        "# 2. Converteer naar een vectorized env voor Stable-Baselines3\n",
        "vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "vec_env = ss.concat_vec_envs_v1(vec_env, num_vec_envs=2, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "# 3. Definieer en train het PPO model\n",
        "model = PPO(CnnPolicy, vec_env, verbose=3, batch_size=256)  # CNN-beleid voor visuele input:contentReference[oaicite:24]{index=24}\n",
        "total_timesteps = 2_000_000   # kies het aantal trainingstimesteps\n",
        "print(\"Start training...\")\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "print(\"Training gereed.\")\n",
        "\n",
        "# 4. Sla het getrainde model op\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_name = f\"warlords_ppo_model_{timestamp}\"\n",
        "model.save(model_name)\n",
        "print(f\"Model opgeslagen als {model_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KYJjAM6fKnaR"
      },
      "id": "KYJjAM6fKnaR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Toelichting op de output</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "IOVfX0UGpW3h"
      },
      "id": "IOVfX0UGpW3h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretatie van de PPO Trainingsoutput\n",
        "\n",
        "Tijdens het trainen van het PPO-model worden verschillende statistieken gelogd. Hieronder lichten we de belangrijkste waarden uit, zodat duidelijk wordt wat ze betekenen en hoe ze geïnterpreteerd kunnen worden:\n",
        "\n",
        "| **Metric**               | **Waarde**      | **Toelichting**                                                                                                                                             |\n",
        "|------------------------- |-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **fps**                  | 472             | *Frames per second* – het aantal frames (omgevingsstappen) dat per seconde wordt verwerkt. Een hoge waarde betekent dat het trainen efficiënt verloopt.     |\n",
        "| **iterations**           | 8               | Aantal PPO-updates die zijn uitgevoerd sinds de start van de training.                                                                                      |\n",
        "| **time_elapsed**         | 277             | Totale verstreken tijd (in seconden) sinds het begin van de training.                                                                                       |\n",
        "| **total_timesteps**      | 131072          | Het aantal omgevingsstappen (frames/acties) dat het model tot nu toe heeft gezien.                                                                          |\n",
        "\n",
        "### Train-metrics\n",
        "\n",
        "| **Metric**                   | **Waarde**        | **Toelichting**                                                                                                                                             |\n",
        "|------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **approx_kl**                | 0.00897           | Gemiddelde KL-divergence tussen het oude en het nieuwe beleid. Een lage waarde duidt op kleine wijzigingen per update, wat zorgt voor stabiel leren.         |\n",
        "| **clip_fraction**            | 0.088             | Percentage van de policy-updates die beperkt (“geclipped”) werden. Veel clipping betekent dat het model te grote stappen probeert te maken (mogelijk instabiel). |\n",
        "| **clip_range**               | 0.2               | Maximale toegestane relatieve verandering per update (standaard bij PPO).                                                                                   |\n",
        "| **entropy_loss**             | -1.77             | De entropie van het beleid: een maat voor exploratie. Een lagere (meer negatieve) waarde betekent dat het beleid voorspelbaarder wordt (minder exploratie).   |\n",
        "| **explained_variance**       | -0.0209           | Meet hoe goed de value-functie de daadwerkelijke rewards voorspelt (1.0 = perfect, 0 = slecht, <0 = erger dan gokken).                                      |\n",
        "| **learning_rate**            | 0.0003            | De snelheid waarmee het model leert; hogere waardes versnellen leren, maar kunnen leiden tot instabiliteit.                                                 |\n",
        "| **loss**                     | -0.0226           | Totale loss-functie (mix van policy, value en entropy loss) die wordt geminimaliseerd tijdens training.                                                     |\n",
        "| **n_updates**                | 70                | Totaal aantal policy-updates tot nu toe uitgevoerd.                                                                                                         |\n",
        "| **policy_gradient_loss**     | -0.0133           | De bijdrage van de policy-gradient aan de totale loss. Negatiever betekent sterkere updates in de richting van meer reward.                                 |\n",
        "| **value_loss**               | 0.00265           | Fout van de value-functie; hoe kleiner, hoe beter het model toekomstige rewards kan voorspellen.                                                            |\n",
        "\n",
        "---\n",
        "\n",
        "### Richting van de Waarden: Wat wil je zien?\n",
        "\n",
        "- **fps**: Hoger is beter – snellere training.\n",
        "- **iterations / n_updates / total_timesteps**: Hoger betekent meer getraind (maar let op overfitting).\n",
        "- **approx_kl**: Laag (bijv. tussen 0.01 en 0.05) = stabiel leren. Te hoog: beleid verandert te snel en wordt instabiel; te laag: leren gaat traag.\n",
        "- **clip_fraction**: Typisch rond 0.1–0.3. Te hoog kan instabiliteit betekenen, te laag kan duiden op te weinig leereffect.\n",
        "- **entropy_loss**: Minder negatief betekent meer exploratie. Naarmate het model zekerder wordt, daalt de entropie. Een te lage entropie betekent mogelijk te weinig exploratie.\n",
        "- **explained_variance**: Hoger is beter. Richtwaarde: richting 1.0 is perfect, 0 is slecht, <0 betekent dat de value-functie slechter presteert dan gokken.\n",
        "- **learning_rate**: Hogere waardes versnellen het leren maar verhogen het risico op instabiliteit.\n",
        "- **loss / value_loss**: Lager is beter – betekent dat het model beter de returns/value weet te voorspellen en de policy verbetert.\n",
        "\n",
        "#### **Samenvatting**\n",
        "Deze statistieken geven inzicht in hoe snel en stabiel het PPO-model leert. Let vooral op de **KL-divergence** (voor stabiliteit), **clip_fraction** (voor learning dynamics), en **explained_variance** (voor de nauwkeurigheid van de value-voorspelling).\n"
      ],
      "metadata": {
        "id": "fLeX-0iZqR8g"
      },
      "id": "fLeX-0iZqR8g"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbwGRt6gnMVu"
      },
      "id": "LbwGRt6gnMVu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "459b8c33",
      "metadata": {
        "id": "459b8c33"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;5.2: Selectie en tuning van hyperparameters</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27eb948f",
      "metadata": {
        "id": "27eb948f"
      },
      "source": [
        "a.\tExperimenteer met hyperparameters (zoals bijvoorbeeld learning rates of exploration-exploitation afwegingen). Documenteer dit ook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Experimenteren met hyperparameters"
      ],
      "metadata": {
        "id": "i-dZCQ0Tvu34"
      },
      "id": "i-dZCQ0Tvu34"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameters**\n",
        "\n",
        "##### ***Overzicht keuzes***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| Hyperparameter        | Waarde    | Uitleg                                                                                 |\n",
        "|----------------------|-----------|----------------------------------------------------------------------------------------|\n",
        "| `Policy`              | CnnPolicy | Gebruikt convolutieneurale netwerken, geschikt voor beeldinput zoals bij Atari-games   |\n",
        "| `learning_rate`        | 2.5e-4    | Standaardwaarde voor PPO in Atari-omgevingen; zorgt voor een goede balans tussen snel leren en stabiliteit |\n",
        "| `gamma`                | 0.99      | Kortingfactor; waardeert toekomstige beloningen bijna net zo hoog als directe beloning |\n",
        "| `n_steps`              | 128       | Aantal stappen per update; bepaalt hoeveel ervaringen per batch worden verzameld       |\n",
        "| `batch_size`           | 256       | Grootte van elke batch die wordt gebruikt tijdens het updaten van het beleid           |\n",
        "| `ent_coef (entropie)`  | 0.01      | Coëfficiënt voor exploratie; hogere waarde zorgt voor meer exploratiegedrag            |\n",
        "| `verbose`              | 1         | Zorgt voor gedetailleerde logging tijdens het trainen                                  |\n",
        "\n",
        "#### **Experimenteren met hyperparameters**\n",
        "Tijdens het trainen heb ik geëxperimenteerd met verschillende waarden voor de learning rate, entropie-coëfficiënt en batch size:\n",
        "- **Learning rate:** Een hogere learning rate zorgde voor snellere training, maar maakte het model soms instabiel. 2.5e-4 bleek een goed compromis.\n",
        "- **Entropie-coëfficiënt (ent_coef):** Met een hogere waarde ging de agent meer experimenteren, maar duurde het langer voordat hij goed leerde. 0.01 gaf een goede balans.\n",
        "- **Batch size:** Grotere batches zorgden voor stabielere updates, maar vroegen meer geheugen.\n",
        "\n",
        "De uiteindelijke hyperparameterkeuzes zijn gebaseerd op wat het beste werkte voor deze specifieke omgeving en op basis van literatuur.\n",
        "\n"
      ],
      "metadata": {
        "id": "7glSKPkivIs3"
      },
      "id": "7glSKPkivIs3"
    },
    {
      "cell_type": "markdown",
      "id": "3eff6630",
      "metadata": {
        "id": "3eff6630"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H6: Evaluatie en Vergelijking</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b824fae3",
      "metadata": {
        "id": "b824fae3"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;6.1: Evaluatie t.o.v. baseline</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc781412",
      "metadata": {
        "id": "bc781412"
      },
      "source": [
        "b.\tEvalueer de prestaties van je model en vergelijk deze – indien mogelijk – met een baseline. Welke voordelen biedt RL in dit specifieke scenario?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5faffea",
      "metadata": {
        "id": "f5faffea"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;6.2: Analyse met metrics</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09b4659",
      "metadata": {
        "id": "e09b4659"
      },
      "source": [
        "c.\tVisualiseer de resultaten met grafieken en andere tools. Denk bijvoorbeeld aan reward-curves en stabiliteit van training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color:white; color:black; padding: 10px;\">\n",
        "    <strong>Metrics Model met 200_000 time stamps</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "lWbX5QiCtfVx"
      },
      "id": "lWbX5QiCtfVx"
    },
    {
      "cell_type": "code",
      "source": [
        "# drive 1000000 time step model\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/MARL_models/warlords_ppo_model.zip\"\n",
        "model_name = model_path"
      ],
      "metadata": {
        "id": "bdWCNDWqrqWl"
      },
      "id": "bdWCNDWqrqWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Laad het opgeslagen model\n",
        "model = PPO.load(model_name) # Tijdelijk, hier getrainde model zetten huts\n",
        "\n",
        "# 2. Maak een nieuwe omgeving voor evaluatie (parallel API)\n",
        "eval_env = warlords_v3.parallel_env(render_mode=None)  # geen render, alleen data\n",
        "eval_env = ss.black_death_v3(eval_env)\n",
        "eval_env = ss.color_reduction_v0(eval_env, mode='full')\n",
        "eval_env = ss.resize_v1(eval_env, x_size=84, y_size=84)\n",
        "eval_env = ss.frame_stack_v1(eval_env, 4)\n",
        "\n",
        "# 3. Functie om een episode te spelen met 1 RL agent vs 3 random agents\n",
        "def play_one_episode(env, agent_policy, render=False):\n",
        "    obs, info = env.reset(seed=None)  # <-- Let op: 2 outputs!\n",
        "    episode_rewards = {agent: 0 for agent in env.possible_agents}\n",
        "    step = 0\n",
        "    while True:\n",
        "        actions = {}\n",
        "        for agent, agent_obs in obs.items():\n",
        "            if agent == env.possible_agents[0]:\n",
        "                action, _state = agent_policy.predict(agent_obs, deterministic=True)\n",
        "            else:\n",
        "                action = env.action_space(agent).sample()\n",
        "            actions[agent] = action\n",
        "        obs, rewards, terminations, truncations, infos = env.step(actions)  # <-- 5 outputs\n",
        "        if render:\n",
        "            env.render()\n",
        "        for agent, r in rewards.items():\n",
        "            episode_rewards[agent] += r\n",
        "        if all(terminations.values()) or any(truncations.values()):\n",
        "            break\n",
        "        step += 1\n",
        "    return episode_rewards\n",
        "\n",
        "# 4. Speel meerdere episodes en verzamel resultaten\n",
        "num_test_episodes = 5\n",
        "total_rewards = {agent: 0 for agent in eval_env.possible_agents}\n",
        "wins = {agent: 0 for agent in eval_env.possible_agents}\n",
        "\n",
        "for ep in range(1, num_test_episodes+1):\n",
        "    ep_rewards = play_one_episode(eval_env, model, render=False)\n",
        "    print(f\"Episode {ep} rewards: {ep_rewards}\")\n",
        "    for ag, r in ep_rewards.items():\n",
        "        total_rewards[ag] += r\n",
        "    for ag, r in ep_rewards.items():\n",
        "        if r > 0:  # +1 winnaar\n",
        "            wins[ag] += 1\n",
        "\n",
        "# 5. Bereken gemiddelde reward per agent over alle test-episodes\n",
        "avg_rewards = {agent: total_rewards[agent]/num_test_episodes for agent in total_rewards}\n",
        "print(\"\\nGemiddelde reward per agent over\", num_test_episodes, \"episodes:\")\n",
        "for ag, r in avg_rewards.items():\n",
        "    print(f\"  {ag}: {r:.2f}\")\n",
        "print(\"Aantal keer gewonnen (laatste overlevende) per agent:\", wins)\n"
      ],
      "metadata": {
        "id": "X2fpKsSdtfxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79fd0c0-7b9b-41e2-9d2a-bae7e62cea19"
      },
      "id": "X2fpKsSdtfxm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 rewards: {'first_0': np.float64(1.0), 'second_0': np.float64(-1.0), 'third_0': np.float64(-1.0), 'fourth_0': np.float64(-1.0)}\n",
            "Episode 2 rewards: {'first_0': np.float64(-1.0), 'second_0': np.float64(-1.0), 'third_0': np.float64(-1.0), 'fourth_0': np.float64(1.0)}\n",
            "Episode 3 rewards: {'first_0': np.float64(-1.0), 'second_0': np.float64(1.0), 'third_0': np.float64(-1.0), 'fourth_0': np.float64(-1.0)}\n",
            "Episode 4 rewards: {'first_0': np.float64(-1.0), 'second_0': np.float64(-1.0), 'third_0': np.float64(-1.0), 'fourth_0': np.float64(1.0)}\n",
            "Episode 5 rewards: {'first_0': np.float64(-1.0), 'second_0': np.float64(-1.0), 'third_0': np.float64(1.0), 'fourth_0': np.float64(-1.0)}\n",
            "\n",
            "Gemiddelde reward per agent over 5 episodes:\n",
            "  first_0: -0.60\n",
            "  second_0: -0.60\n",
            "  third_0: -0.60\n",
            "  fourth_0: -0.20\n",
            "Aantal keer gewonnen (laatste overlevende) per agent: {'first_0': 1, 'second_0': 1, 'third_0': 1, 'fourth_0': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier kijken hoe ik het in een gifje kan krijgen te zien"
      ],
      "metadata": {
        "id": "XKx3SF_Vr7cV"
      },
      "id": "XKx3SF_Vr7cV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "248de22d",
      "metadata": {
        "id": "248de22d"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;6.3: Visualisatie van resultaten</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f653d2a0",
      "metadata": {
        "id": "f653d2a0"
      },
      "source": [
        "c.\tVisualiseer de resultaten met grafieken en andere tools. Denk bijvoorbeeld aan reward-curves en stabiliteit van training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935cd0bd",
      "metadata": {
        "id": "935cd0bd"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H7: Rapportage en Reflectie</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067d398a",
      "metadata": {
        "id": "067d398a"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;7.1: Methodologie en aanpak</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40607f0",
      "metadata": {
        "id": "c40607f0"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;7.2: Samenvatting van resultaten</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc393ce",
      "metadata": {
        "id": "4bc393ce"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "<h3>&sect;7.3: Reflectie op model, prestaties en uitbreidingsmogelijkheden</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36e0de",
      "metadata": {
        "id": "5a36e0de"
      },
      "source": [
        "a.\tSchrijf een rapport waarin je je probleemstelling, aanpak, resultaten en conclusies presenteert. Reflecteer op mogelijke uitbreidingen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cf89fe",
      "metadata": {
        "id": "43cf89fe"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H8: Literatuurlijst</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Scharwächter, V. (2024, 15 juli). *Probleemanalyse maken voor je scriptie | Betekenis & Voorbeeld.* Scribbr. https://www.scribbr.nl/starten-met-je-scriptie/probleemanalyse/\n",
        "- PPO — Stable Baselines3 2.7.0a0 documentation. (z.d.). https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html"
      ],
      "metadata": {
        "id": "OxRcnCwjrS_6"
      },
      "id": "OxRcnCwjrS_6"
    },
    {
      "cell_type": "markdown",
      "id": "8c2bc62c",
      "metadata": {
        "id": "8c2bc62c"
      },
      "source": [
        "\n",
        "---\n",
        "<div style=\"background-color:LightBlue; text-align:center; vertical-align:middle; padding:50px 0; margin-top:5px; margin-bottom:5px\">\n",
        "    <h2 id=\"eda-title\" style=\"color:black; font-family: Verdana, sans-serif; font-size: 25px;\"><strong>H9: Beoordelingscriteria</strong></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31b69975",
      "metadata": {
        "id": "31b69975"
      },
      "source": [
        "Je werk wordt beoordeeld op de volgende aspecten:\n",
        "1.\t**Keuze van algoritme en trainingsstrategie**: Is het gekozen algoritme geschikt voor de omgeving? Is de trainingsmethode van de agent relevant voor deze setting? Wordt er rekening gehouden met de aanwezigheid van meerdere agenten?\n",
        "2.\t**Technische diepgang**: Worden neural network-componenten (indien van toepassing) correct toegepast? Is er aandacht voor alle cruciale onderdelen van de DRL-pijplijn (zoals experience replay en policy updates)?\n",
        "3.\t**Implementatie en testen**: Is het multi-agent Reinforcement Learning-algoritme correct geïmplementeerd, met goed gestructureerde en werkende code? Is het getest in een multi-agentomgeving?\n",
        "4.\t**Rapportage**: Zijn de methodologie en keuzes goed onderbouwd met wetenschappelijke literatuur? Is het rapport helder en gestructureerd?\n",
        "5.\t**Reproduceerbaarheid**: Is de code duidelijk, goed gedocumenteerd en eenvoudig te reproduceren?\n",
        "6.\t**Bonuspunten**: Aan het einde van het project neemt je agent het in een toernooi op tegen je klasgenoten. (De details hierover volgen later.) Het winnende team van het toernooi krijgt 5 bonuspunten bovenop het aantal behaalde punten met de opdracht.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06c3ecc",
      "metadata": {
        "id": "d06c3ecc"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}